# ğŸ§  GPTMini â€“ A Mini GPT Clone from Scratch

Welcome to **GPTMini**, a transformer-based language model built **entirely from scratch using PyTorch**!  
This project is a minimal, educational implementation of GPT-style models that helped me understand the internals of transformer architectures deeply.

---

## ğŸ“Œ Features

- âœ”ï¸ Tokenizer integration
- âœ”ï¸ Positional Encoding
- âœ”ï¸ Input Embedding
- âœ”ï¸ Self-Attention mechanism
- âœ”ï¸ Residual connections + LayerNorm
- âœ”ï¸ FeedForward network
- âœ”ï¸ Stacked Transformer blocks
- âœ”ï¸ Full training pipeline from scratch
- âœ”ï¸ Text generation with sampling

---

## ğŸ§± Architecture Overview

Text â†’ Tokenizer â†’ Input Embedding â†’
â†’ [TransformerBlock Ã— N] â†’
â†’ Linear layer (vocab projection) â†’
â†’ Generated Output



Each `TransformerBlock` includes:
- Self-Attention â†’ Residual â†’ LayerNorm  
- FeedForward â†’ Residual â†’ LayerNorm

---

## ğŸ“‰ Training Log

Epoch 1/300, Loss: 134.3330
Epoch 51/300, Loss: 4.2212
Epoch 101/300, Loss: 1.1362
Epoch 151/300, Loss: 0.3862
Epoch 201/300, Loss: 0.3172
Epoch 251/300, Loss: 0.3834
Epoch 300/300, Loss: 0.2838


---

## ğŸ”¥ Sample Output

HHHHHa! and then the story continues with randomness based on data...



---

## ğŸ› ï¸ How to Run

```bash
# Clone the repo
git clone https://github.com/241001076/GPT-From-Scratch.git
cd GPT-From-Scratch

# Make sure you have Python and PyTorch installed
pip install torch numpy

# Run the training
python new.py


                                                                                              Engineered by Harish Akshay H
